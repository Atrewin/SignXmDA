# Cross-modality Data Augmentation for Sign Language Translation

Implementation of our paper [XmDA: Cross-modality Data Augmentation for End-to-End
Sign Language Translation](https://arxiv.org/pdf/2305.11096.pdf). 


## Brief Introduction
we propose a novel Cross-modality Data Augmentation (XmDA) approach to improve the end-to-end SLT performance. The main idea of XmDA is to leverage the powerful gloss-to-text translation capabilities (unimode, i.e., text-to-text) to end-to-end sign language translation (cross mode, i.e., video-to-text). Specifically, XmDA integrates two techniques, namely Cross-modality Mix-up and Cross-modality Knowledge Distillation (KD).
The Cross-modality Mix-up technique combines sign language video features with gloss embeddings extracted from the gloss-to-text teacher model to generate mixed-modal augmented samples. Concurrently,  the Cross-modality KD utilizes diversified spoken language texts generated by the powerful gloss-to-text teacher models to soften the target labels, thereby further diversifying and enhancing the augmented samples. 

<div align="center">
    <img src="/images/framework.jpg" width="80%" title="The overall framework of cross-modality data augmentation." </img>
    <p class="image-caption">Figure 1: The overall framework of cross-modality data augmentation methods for SLT in this work.  Components in gray indicate frozen parameters.</p>
</div>



## Reference Performance
We conduct evaluations for the proposed XmDA approach on End-to-end SLT performance and analysis the sign representation
distributions on PHOENIX-2014T dataset.

### End-to-end SLT performance on PHOENIX-2014T dataset

<div align="center">
    <img src="/images/results_dsl.jpg" width="70%" title="Framework of Self-training with Uncertainty-Based Sampling."</img>
</div>

[//]: # (    <p class="image-caption">Table 1: End-to-end SLT performance on PHOENIX-2014T dataset. “+ XmDA” denotes the application of both)
[//]: # (Cross-modality Mix-up and Cross-modality KD.</p>)



### The topological structure of input embeddings 

<div align="center">
    <img src="/images/embeddings.jpg" width="70%" title="Framework of Self-training with Uncertainty-Based Sampling."</img>
</div>


## Implementation Guidelines


* This code is based on [Sign Language Transformers](https://github.com/neccam/slt) but modified to realize Cross-modality KD and  Cross-modality mix-up. 
* For baseline end-to-end SLT, you can use the [Sign Language Transformers](https://github.com/neccam/slt). 
* For gloss-to-text tearchers model, you can follow the [PGen](https://github.com/Atrewin/PGen) or use the original text-to-text [Joey NMT](https://github.com/joeynmt/joeynmt) framework. 
* For put them to one, we expend Sign Language Transformers framework with Joey NMT and allow the new one can forward gloss-to-text and mix-to-text (i.e., ``forward_type`` in [sign, gloss, mixup]).


### Requirements
* Create environment follow [Sign Language Transformers](https://github.com/neccam/slt).
* Reproduce [PGen](https://github.com/Atrewin/PGen) to obtain multi-references as sentence-level guidance from gloss-to-text teachers (or using ``forward_type`` = gloss).
* Reproduce [SMKD](https://github.com/ycmin95/VAC_CSLR) to pre-process the sign video.
* Pre-process dataset and put them into ``./data/DATA-NAME/`` (ref the format to https://github.com/neccam/slt)


### Usage

  `python -m signjoey train_XmDA configs/Sign_XmDA.yaml` 

! Note that the default data directory is `./data`. If you download them to somewhere else, you need to update the `data_path` parameters in your config file.   
### ToDo:

- [X] *Initial code release.*
- [X] Release Pre-process dataset.
- [ ] Share extensive qualitative and quantitative results & config files to generate them.


### Reference

Please cite the paper below if you use this code in your research:

    @article{ye2023cross,
      title={Cross-modality Data Augmentation for End-to-End Sign Language Translation},
      author={Ye, Jinhui and Jiao, Wenxiang and Wang, Xing and Tu, Zhaopeng and Xiong, Hui},
      journal={arXiv preprint arXiv:2305.11096},
      year={2023}
    }


